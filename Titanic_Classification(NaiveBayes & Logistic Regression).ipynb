{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective is to find whether a person is Survived or not in Titanic accident depending upon the given features\n",
    "# We're going to use Naive Bayes and Logistic regression to compare which is best for Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE : explanation for Naive bayes is not given but logistic regression is given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/adity/titanic_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping NaN values & Unwanted columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Name','Ticket','Cabin','PassengerId'],axis=1)\n",
    "df = df.dropna(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making target and Features variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df['Survived'].copy()\n",
    "features = df.drop(['Survived'],axis = 1).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Sex and Embarked(As they're textual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['Sex'] = features['Sex'].replace(['male','female'],[1,0])\n",
    "features['Embarked'] = features['Embarked'].replace(['C','S','Q'],[1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Features : (712, 7)\n",
      "Shape of Target : (712,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Features :',features.shape)\n",
    "print('Shape of Target :',target.shape)\n",
    "######### SHOULD FIX THE TARGET SHAPE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(target)\n",
    "target = target.reshape(1,len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Features : (712, 7)\n",
      "Shape of Target : (1, 712)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Features :',features.shape)\n",
    "print('Shape of Target :',target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_pos = features[df['Survived']==1].cov()\n",
    "cov_neg = features[df['Survived']==0].cov()\n",
    "\n",
    "######## Both Give shape 7x7 ###########\n",
    "################ CAUTION ##############\n",
    "###### DO NOT USE NP.COV AS IT MAY RESULT IN INCORRECT CO-VARIANCE #######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_pos = np.mean(features[df['Survived']==1])\n",
    "mean_neg = np.mean(features[df['Survived']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7,)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_pos.shape\n",
    "######### DO NOT WORRY ABOUT SHAPE, STILL IT GIVES PROPER RESULTS #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_pos = len(features[df['Survived']==1])/len(features)\n",
    "p_neg = len(features[df['Survived']==0])/len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(712, 7)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(features-mean_pos).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.inv(cov_pos).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_pos(x, mean_pos, cov_pos):\n",
    "    return (1/(((2*np.pi)**len(features.T))*np.linalg.det(cov_pos))**0.5)*np.exp((-0.5)*np.dot(np.dot((x-mean_pos) ,np.linalg.inv(cov_pos)),(x-mean_pos).T))\n",
    "def prior_neg(x, mean_neg, cov_neg):\n",
    "    return (1/(((2*np.pi)**len(features.T))*np.linalg.det(cov_neg))**0.5)*np.exp((-0.5)*np.dot(np.dot((x-mean_neg) ,np.linalg.inv(cov_neg)),(x-mean_neg).T))\n",
    "def posterior_pos(x, prior_pos, prior_neg, p_pos, p_neg):\n",
    "    return ((prior_pos(x, mean_pos, cov_pos))*p_pos)/(((prior_pos(x, mean_pos, cov_pos))*p_pos)+((prior_neg(x, mean_neg, cov_neg))*p_neg))\n",
    "def posterior_neg(x, prior_pos, prior_neg, p_pos, p_neg):\n",
    "    return ((prior_neg(x, mean_neg, cov_neg))*p_neg)/(((prior_pos(x, mean_pos, cov_pos))*p_pos)+((prior_neg(x, mean_neg, cov_neg))*p_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## we are doing FEATURES.ILOC[i] to get post_pos value of each instance ###########\n",
    "\n",
    "pred_class = []\n",
    "for i in range(len(features)):\n",
    "    q = (posterior_pos(features.iloc[i], prior_pos, prior_neg, p_pos, p_neg))\n",
    "    r = (posterior_neg(features.iloc[i], prior_pos, prior_neg, p_pos, p_neg))\n",
    "    if(q > r):\n",
    "        pred_class.append(1)\n",
    "    else:\n",
    "        pred_class.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.DataFrame(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predicted,actual):\n",
    "  correct = 0\n",
    "  for i in range(0,len(predicted)):\n",
    "    if(predicted[i]==actual[i][0]):\n",
    "      correct += 1\n",
    "  return (correct/len(predicted))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.19662921348315"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(pred_class,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### USELESS FEATURE ########\n",
    "features = features.drop(['SibSp'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.array(target)\n",
    "target = target.reshape(1,712)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features : (712, 6)\n",
      "Shape of Target : (1, 712)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of features :',features.shape)\n",
    "print('Shape of Target :',target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NORMALISING is important to avoid any errors and to achieve good accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features/features.max()\n",
    "target = target/target.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Functions\n",
    "\n",
    "# This is Hypothesis function in Logistic Regression, \n",
    "## $$ p=\\frac{1}{1+e^{-(mx+c)}} $$\n",
    "### This gives values ranging from 0 to 1 which we're interested to get as our target var ranges from 0 to 1\n",
    "### We are going to predict values using MLE instead of MSE as it gives very high error values\n",
    "### First, we'll be finding log(p) for those values whose target var value is 1 and also find log(1-p) for those values whose target var value is 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By doing log, we get values ranging from -inf to +inf,\n",
    "##  (i.e) $$ log(\\frac{p}{1-p}) = \\frac{log(\\frac{1}{1+e^{-(mx+c)}})}{1+\\frac{1}{1+e^{-(mx+c)}}}  $$\n",
    "# We call this as \"ODDS FUNCTION\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For given some m & c values,\n",
    "## if we compute log(p), we get some corresponing value in log(odds) graph.\n",
    "## where log(p) is for those values whose target_var value is 1 and, similarly for log(1-p) , we get some value in log(odds) graph where log(1-p) is for those values whose target_var value is 0\n",
    "## those values denotes our cost "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our goal is to reduce m and c values such that with good m and c, the \"p\" will be reduced and hence log will be reduced\n",
    "## \"\"\"\"\"\"\"\"\"\"\"Make sure log shouldn't return 1, if did, then, error becomes infinity\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(m,x,c):\n",
    "    return 1/(1+np.exp(-(np.dot(m,x.T)+c)))\n",
    "def cost(m,x,c,y):\n",
    "    pred_class = []\n",
    "    sig = sigmoid(m,x,c)\n",
    "    d = 0.0005\n",
    "    for i in range(len(x)):\n",
    "        if(y[0][i] == 1):\n",
    "            pred_class.append(-np.log(sigmoid(m,x.iloc[i],c)+d))  #### We add d to avoid values to go to 0.9999999 which may return log value to inf\n",
    "        else:\n",
    "            pred_class.append(-np.log(1-sigmoid(m,x.iloc[i],c)+d))\n",
    "    pred_class = np.array(pred_class)\n",
    "    pred_class = pred_class.reshape(1,712)   #For BroadCasting purposes\n",
    "    return np.mean((pred_class))\n",
    "def grad_m(m,x,c,y):\n",
    "    grad= np.mean((sigmoid(m,x,c)-y)*x.T,axis=1)  #### Make sure formula is (yp - ya)\n",
    "    grad = np.array(grad)\n",
    "    grad = grad.reshape(1,6)\n",
    "    return grad\n",
    "def grad_c(m,x,c,y):\n",
    "    return np.mean((sigmoid(m,x,c)-y))\n",
    "def accuracy(m,x,c,y):\n",
    "    y_pred=sigmoid(m,x,c)\n",
    "    y_pred[y_pred>=0.5]=1   ### As y_pred in our case can be any value between 0 to 1\n",
    "    y_pred[y_pred<0.5]=0\n",
    "    correct=len(y_pred[y_pred==y])\n",
    "    return correct/len(y.T)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = np.random.randn(1,6)\n",
    "c = random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : 0.4490761703874182 Iteration : 999 Accuracy : 79.7752808988764\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error = []\n",
    "lr = 5   #### Already values are very small , and if lr is small, then it takes plenty of time\n",
    "for i in range(1000):\n",
    "    m = m-lr*grad_m(m,features,c,target)\n",
    "    c = c-lr*grad_c(m,features,c,target)\n",
    "    error.append(cost(m,features,c,target))\n",
    "    clear_output(True)\n",
    "    print('Error :',cost(m,features,c,target),'Iteration :',i,'Accuracy :',accuracy(m,features,c,target))\n",
    "plt.plot(error)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion : Naive Bayes is better for classifying Titanic Dataset than Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
